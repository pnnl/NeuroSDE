{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d4a2a7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb14e9a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "246bc267",
   "metadata": {},
   "source": [
    "** Rough Draft of Neuromancer UTIntegrator and sigma points / inverse sigma points Nodes**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37221f3a",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Refer to `simulators.py` and `datasets.py` to generate time-series datasets of moments and create sigma points dataset. These are for PyTorch, however. Trying to find a way to handle these in Neuromancer. \n",
    "\n",
    "To be able to showcase the SPINODE method on existing Neuromancer examples below is a code to run PSL systems, generate Monte Carlo simulations, and then add moments initial condition and rollout to the DictDataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3b01a2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# ---- Euler-Murayama step for generic ODE_Autonomous drift: x_{k+1} = x_k + f(x_k,t_k)*dt + sigma*sqrt(dt)*eta ----\n",
    "def em_rollout_system(sys, T, dt, sigma=0.0, x0=None, seed=None):\n",
    "    \"\"\"\n",
    "    sys: an instance of ODE_Autonomous (from neuromancer.psl.base)\n",
    "    T:   number of integration steps\n",
    "    dt:  time step\n",
    "    sigma: scalar noise std (additive, isotropic); 0 => deterministic\n",
    "    x0:  initial state (1D array, length nx). If None, uses sys default x0.\n",
    "    returns: ndarray [T+1, nx]\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        rng = np.random.default_rng(seed)\n",
    "    else:\n",
    "        rng = np.random.default_rng()\n",
    "\n",
    "    # use system's default initial condition if not provided\n",
    "    if x0 is None:\n",
    "        # sys.params defines variables/constants/parameters; variables['x0'] is default init\n",
    "        x0 = np.array(sys.x0, dtype=float)\n",
    "    else:\n",
    "        x0 = np.array(x0, dtype=float)\n",
    "\n",
    "    nx = x0.size\n",
    "    X = np.zeros((T+1, nx), dtype=float)\n",
    "    X[0] = x0\n",
    "\n",
    "    # EM time loop\n",
    "    t = 0.0\n",
    "    for k in range(T):\n",
    "        # drift f(x,t) via the system's equations\n",
    "        f_list = sys.equations(t, X[k])        # returns list of derivatives\n",
    "        f = np.array(f_list, dtype=float)       # [nx]\n",
    "        # additive noise\n",
    "        noise = sigma * np.sqrt(dt) * rng.standard_normal(size=nx)\n",
    "        # EM update\n",
    "        X[k+1] = X[k] + f*dt + noise\n",
    "        t += dt\n",
    "    return X\n",
    "\n",
    "class DictDataset4D:\n",
    "    \"\"\"\n",
    "    Minimal dict-style dataset compatible with your DataLoader pattern.\n",
    "    Stores tensors with shape:\n",
    "      X:  [B, T, nx, E]\n",
    "      x0: [B, 1, nx, E]  (optional convenience key)\n",
    "    \"\"\"\n",
    "    def __init__(self, arrays: dict, name='dataset'):\n",
    "        self.arrays = {k: torch.as_tensor(v).float() for k, v in arrays.items()}\n",
    "        self.name = name\n",
    "        self.N = self.arrays['X'].shape[0]  # B\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.N\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {k: v[idx] for k, v in self.arrays.items()}\n",
    "\n",
    "    # simple collate: stack dict entries along batch\n",
    "    @staticmethod\n",
    "    def collate_fn(batch_list):\n",
    "        out = {}\n",
    "        keys = batch_list[0].keys()\n",
    "        for k in keys:\n",
    "            out[k] = torch.stack([b[k] for b in batch_list], dim=0)\n",
    "        return out\n",
    "def get_mc_data(system_class,\n",
    "                nsteps_total: int,\n",
    "                nsteps_per_batch: int,\n",
    "                dt: float,\n",
    "                batch_size: int,\n",
    "                nx: int = None,\n",
    "                experiments: int = 64,\n",
    "                sigma: float = 0.05,\n",
    "                x0_sampler=None,\n",
    "                seed: int = 0):\n",
    "    \"\"\"\n",
    "    Build train/dev/test with a 4-D array: [B, T, nx, E]\n",
    "      - system_class: one of your systems (e.g., VanDerPol)\n",
    "      - nsteps_total: total rollout length to generate per split\n",
    "      - nsteps_per_batch: T for each batch item (segment length)\n",
    "      - dt: time step\n",
    "      - batch_size: DataLoader batch size\n",
    "      - experiments: E (number of MC particles per segment)\n",
    "      - sigma: additive noise std for EM\n",
    "      - x0_sampler: function (E)-> array [E, nx] for initial states; default = system default, jittered\n",
    "      - seed: RNG seed\n",
    "\n",
    "    Returns:\n",
    "      train_loader, dev_loader, test_data (dict with X: [1, nsteps_total, nx, E], x0)\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    # instantiate the system\n",
    "    sys = system_class()\n",
    "    if nx is None:\n",
    "        nx = len(np.array(sys.x0, dtype=float))\n",
    "\n",
    "    # default x0 sampler: jitter around system's default\n",
    "    if x0_sampler is None:\n",
    "        def x0_sampler(E):\n",
    "            base = np.array(sys.x0, dtype=float)\n",
    "            jitter = 0.05 * rng.standard_normal(size=(E, nx))\n",
    "            return base[None, :] + jitter\n",
    "\n",
    "    # --- helper: ensemble rollout to [T+1, nx, E] ---\n",
    "    def ensemble_rollout(T, E):\n",
    "        X_all = np.zeros((T+1, nx, E), dtype=float)\n",
    "        X0s = x0_sampler(E)                              # [E, nx]\n",
    "        for e in range(E):\n",
    "            Xi = em_rollout_system(sys, T=T, dt=dt, sigma=sigma, x0=X0s[e], seed=rng.integers(1<<30))\n",
    "            X_all[:, :, e] = Xi\n",
    "        return X0s, X_all\n",
    "\n",
    "    # --- make three splits ---\n",
    "    def make_split(nsteps_total, name='split'):\n",
    "        X0s, X_all = ensemble_rollout(T=nsteps_total, E=experiments)  # X_all: [T+1, nx, E]\n",
    "        # chop into batches of length nsteps_per_batch\n",
    "        T_full = nsteps_total\n",
    "        nbatch = T_full // nsteps_per_batch\n",
    "        T_used = nbatch * nsteps_per_batch\n",
    "\n",
    "        # reshape: [B, T, nx, E]\n",
    "        X_used = X_all[:T_used+1]  # keep one extra for convenience if needed\n",
    "        # we’ll use non-overlapping windows [k:k+T]\n",
    "        X_batches = []\n",
    "        x0_batches = []\n",
    "        for b in range(nbatch):\n",
    "            start = b * nsteps_per_batch\n",
    "            stop  = start + nsteps_per_batch\n",
    "            Xb = X_used[start:stop]             # [T, nx, E]\n",
    "            x0b = X_used[start:start+1]         # [1, nx, E]\n",
    "            X_batches.append(np.transpose(Xb, (0,1,2)))     # keep as [T, nx, E]\n",
    "            x0_batches.append(np.transpose(x0b, (0,1,2)))   # [1, nx, E]\n",
    "\n",
    "        X_arr  = np.stack(X_batches,  axis=0)   # [B, T, nx, E]\n",
    "        x0_arr = np.stack(x0_batches, axis=0)   # [B, 1, nx, E]\n",
    "\n",
    "        data = DictDataset4D({'X': X_arr, 'x0': x0_arr}, name=name)\n",
    "        loader = DataLoader(data, batch_size=batch_size, shuffle=True,\n",
    "                            collate_fn=DictDataset4D.collate_fn)\n",
    "        return data\n",
    "\n",
    "    # Train / Dev / Test (independent draws by changing seed streams)\n",
    "    train_data = make_split(nsteps_total, name='train')\n",
    "    dev_data = make_split(nsteps_total, name='dev')\n",
    "    \n",
    "\n",
    "    return train_data, dev_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d23c976",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Example: build a 4-D MC dataset for VanDerPol\n",
    "from types import SimpleNamespace\n",
    "\n",
    "from neuromancer import psl\n",
    "system = psl.systems['VanDerPol']\n",
    "\n",
    "# Assume your VanDerPol class above is imported\n",
    "train_data, dev_data = get_mc_data(\n",
    "    system_class=system,\n",
    "    nsteps_total=2000,      # total steps per split\n",
    "    nsteps_per_batch=200,   # window length T\n",
    "    dt=0.01,\n",
    "    batch_size=16,\n",
    "    experiments=64,         # E particles\n",
    "    sigma=0.05,             # additive noise\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Peek at one batch\n",
    "batch = next(iter(train_loader))\n",
    "for k, v in batch.items():\n",
    "    print(k, v.shape)       # X: [B, T, nx, E], x0: [B, 1, nx, E]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa69854f",
   "metadata": {},
   "source": [
    "## Neuromancer Codes\n",
    "\n",
    "Please see `create_modular_ut_system`. Data flow is \n",
    "```\n",
    "mu, var, u → sigma_points, W → sigma_points_next → mu_next, var_next\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e3edb8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import neuromancer as nm\n",
    "from neuromancer.system import Node, System\n",
    "from neuromancer.modules import blocks\n",
    "from neuromancer.dataset import DictDataset\n",
    "from neuromancer.constraint import variable\n",
    "from neuromancer.loss import PenaltyLoss\n",
    "from neuromancer.problem import Problem\n",
    "from neuromancer.trainer import Trainer\n",
    "from neuromancer.dynamics.integrators import DiffEqIntegrator\n",
    "\n",
    "# ==============================================================================\n",
    "# UTILITY FUNCTIONS\n",
    "# ==============================================================================\n",
    "\n",
    "def cholesky_psd(A: torch.Tensor, eps: float = 1e-12) -> torch.Tensor:\n",
    "    I = torch.eye(A.shape[-1], device=A.device, dtype=A.dtype)\n",
    "    return torch.linalg.cholesky(A + eps * I)\n",
    "\n",
    "def ut_weights_torch(n: int, alpha=1e-3, beta=2.0, kappa=0.0,\n",
    "                     device=\"cpu\", dtype=torch.float32) -> Tuple[float, torch.Tensor, torch.Tensor]:\n",
    "    lam = alpha**2 * (n + kappa) - n\n",
    "    Wm = torch.zeros(2*n+1, device=device, dtype=dtype)\n",
    "    Wc = torch.zeros(2*n+1, device=device, dtype=dtype)\n",
    "    Wm[0] = lam/(n+lam)\n",
    "    Wc[0] = lam/(n+lam) + (1 - alpha**2 + beta)\n",
    "    Wm[1:] = Wc[1:] = 1.0/(2*(n+lam))\n",
    "    return lam, Wm, Wc\n",
    "\n",
    "# ==============================================================================\n",
    "# UTINTEGRATOR (YOUR CODE)\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "\n",
    "class UTIntegrator(DiffEqIntegrator):\n",
    "    \"\"\"Extended DiffEqIntegrator for Unscented Transform (UT) integration.\"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 block_rhs: nn.Module,\n",
    "                 dt: float = 0.01,\n",
    "                 method: str = 'dopri5',\n",
    "                 rtol: float = 1e-5,\n",
    "                 atol: float = 1e-7,\n",
    "                 sigma_val: float = 0.1,\n",
    "                 include_diffusion: bool = True,\n",
    "                 nx: int = 1,\n",
    "                 nu: int = 1,\n",
    "                 interp_u=None):\n",
    "        \n",
    " \n",
    "        super().__init__(block=block_rhs, interp_u=interp_u, h=dt, method=method, rtol=rtol, atol=atol)\n",
    "        \n",
    "        self.sigma_val = sigma_val\n",
    "        self.include_diffusion = include_diffusion\n",
    "        self.nx = nx\n",
    "        self.nu = nu\n",
    "        \n",
    "    def integrate(self, sigma_points: torch.Tensor, *args) -> torch.Tensor:\n",
    "        \"\"\"Integrate sigma points through dynamics.\"\"\"\n",
    "        B, S, D = sigma_points.shape\n",
    "        device, dtype = sigma_points.device, sigma_points.dtype\n",
    "        \n",
    "        ptr = 0\n",
    "        x = sigma_points[..., ptr:ptr + self.nx]  # [B, S, nx]\n",
    "        ptr += self.nx\n",
    "        u = sigma_points[..., ptr:ptr + self.nu] if self.nu > 0 else None  # [B, S, nu] or None\n",
    "        ptr += self.nu\n",
    "        w = sigma_points[..., ptr:ptr + self.nx] if self.include_diffusion else None  # [B, S, nx] or None\n",
    "        \n",
    "        # Flatten for batch integration\n",
    "        x_flat = x.reshape(-1, self.nx)  # [B*S, nx]\n",
    "        u_flat = u.reshape(-1, self.nu) if u is not None else None  # [B*S, nu] or None\n",
    "        \n",
    "        # Use base integrate method\n",
    "        x_next_flat = super().integrate(x_flat, u_flat) if u_flat is not None else super().integrate(x_flat)\n",
    "        x_next = x_next_flat.view(B, S, self.nx)  # [B, S, nx]\n",
    "        \n",
    "        if self.include_diffusion:\n",
    "            sigma_val = torch.as_tensor(self.sigma_val, device=device, dtype=dtype)\n",
    "            if sigma_val.ndim == 0:\n",
    "                sigma_val = sigma_val.repeat(self.nx)\n",
    "            g2 = 0.5 * (sigma_val ** 2)  # [nx]\n",
    "            sqrt_term = torch.sqrt(2.0 * g2 * self.h)  # [nx]\n",
    "            diffusion_term = sqrt_term[None, None, :] * w  # [B, S, nx]\n",
    "            x_next = x_next + diffusion_term\n",
    "        \n",
    "        return x_next  # [B, S, nx]\n",
    "\n",
    "# ==============================================================================\n",
    "# SIGMA POINT NODES\n",
    "# ==============================================================================\n",
    "\n",
    "class SigmaPointGeneratorNode(nn.Module):\n",
    "    \"\"\"Generate sigma points from mean and variance.\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 include_diffusion: bool = True,\n",
    "                 alpha: float = 1e-3,\n",
    "                 beta: float = 2.0, \n",
    "                 kappa: float = 0.0):\n",
    "        \n",
    "        super().__init__(nn.Identity(), input_keys, output_keys, name='sigma_generator')\n",
    "        self.include_diffusion = include_diffusion\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.kappa = kappa\n",
    "        \n",
    "    def aug_sigma_1d_xw(self, mu: torch.Tensor, var: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Generate augmented sigma points for 1D state + 1D noise.\"\"\"\n",
    "        device, dtype = mu.device, mu.dtype\n",
    "        nx = nw = 1; n = nx + nw\n",
    "        lam, Wm, _ = ut_weights_torch(n, self.alpha, self.beta, self.kappa, device, dtype)\n",
    "        \n",
    "        Saug = torch.zeros(n, n, device=device, dtype=dtype)\n",
    "        Saug[0, 0] = var  # state variance\n",
    "        Saug[1, 1] = 1.0  # unit noise variance\n",
    "        \n",
    "        L = cholesky_psd((n + lam) * Saug)\n",
    "        S = torch.zeros(2*n+1, 2, device=device, dtype=dtype)\n",
    "        \n",
    "        S[0, 0] = mu; S[0, 1] = 0.0\n",
    "        for i in range(n):\n",
    "            S[1+i,    0] = mu + L[0, i]; S[1+i,    1] =  L[1, i]\n",
    "            S[1+n+i,  0] = mu - L[0, i]; S[1+n+i,  1] = -L[1, i]\n",
    "            \n",
    "        return S, Wm\n",
    "    \n",
    "    def det_sigma_copy(self, mu_vec: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"Generate deterministic sigma points.\"\"\"\n",
    "        nx = mu_vec.shape[-1]\n",
    "        _, Wm, _ = ut_weights_torch(nx, self.alpha, self.beta, self.kappa, \n",
    "                                  mu_vec.device, mu_vec.dtype)\n",
    "        S = mu_vec.unsqueeze(-2).repeat([1] * (mu_vec.dim()-1) + [2*nx+1, 1])\n",
    "        return S, Wm\n",
    "        \n",
    "    def forward(self, data: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "        mu = data[self.input_keys[0]]   # [B, nx]\n",
    "        var = data[self.input_keys[1]]  # [B, nx]\n",
    "        u = data[self.input_keys[2]]    # [B, nu]\n",
    "        \n",
    "        batch_size = mu.shape[0]\n",
    "        \n",
    "        if mu.shape[-1] == 1:  # 1D case\n",
    "            sigma_points_list = []\n",
    "            W_list = []\n",
    "            \n",
    "            for b in range(batch_size):\n",
    "                if self.include_diffusion:\n",
    "                    S, Wm = self.aug_sigma_1d_xw(mu[b, 0], var[b, 0])  # [S, 2]\n",
    "                    x_sigma = S[:, 0:1]  # [S, 1]\n",
    "                    w_sigma = S[:, 1:2]  # [S, 1]\n",
    "                    u_col = u[b].unsqueeze(0).repeat(S.shape[0], 1)  # [S, 1]\n",
    "                    sigma_pts = torch.cat([x_sigma, u_col, w_sigma], dim=-1)  # [S, 3]\n",
    "                else:\n",
    "                    S, Wm = self.det_sigma_copy(mu[b:b+1])  # [1, S, 1]\n",
    "                    S = S.squeeze(0)  # [S, 1]\n",
    "                    u_col = u[b].unsqueeze(0).repeat(S.shape[0], 1)  # [S, 1]\n",
    "                    sigma_pts = torch.cat([S, u_col], dim=-1)  # [S, 2]\n",
    "                \n",
    "                sigma_points_list.append(sigma_pts)\n",
    "                W_list.append(Wm)\n",
    "            \n",
    "            sigma_points = torch.stack(sigma_points_list, dim=0)  # [B, S, D]\n",
    "            W = torch.stack(W_list, dim=0)  # [B, S]\n",
    "            \n",
    "        else:  # Multi-dimensional deterministic\n",
    "            sigma_points_list = []\n",
    "            W_list = []\n",
    "            \n",
    "            for b in range(batch_size):\n",
    "                S, Wm = self.det_sigma_copy(mu[b:b+1])  # [1, S, nx]\n",
    "                S = S.squeeze(0)  # [S, nx]\n",
    "                u_col = u[b].unsqueeze(0).repeat(S.shape[0], 1)  # [S, nu]\n",
    "                sigma_pts = torch.cat([S, u_col], dim=-1)  # [S, nx+nu]\n",
    "                \n",
    "                sigma_points_list.append(sigma_pts)\n",
    "                W_list.append(Wm)\n",
    "            \n",
    "            sigma_points = torch.stack(sigma_points_list, dim=0)  # [B, S, nx+nu]\n",
    "            W = torch.stack(W_list, dim=0)  # [B, S]\n",
    "        \n",
    "        return sigma_points, W\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "class InverseSigmaPointNode(nn.Module):\n",
    "    \"\"\"Compute mean and variance from weighted sigma points.\"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 compute_variance: bool = False):\n",
    "        \n",
    "        if compute_variance:\n",
    "            output_keys = output_keys + ['var_next']\n",
    "            \n",
    "        super().__init__(nn.Identity(), input_keys, output_keys, name='inverse_sigma')\n",
    "        self.compute_variance = compute_variance\n",
    "        \n",
    "    def forward(self, data: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
    "        sigma_points_next = data[self.input_keys[0]]  # [B, S, nx]\n",
    "        W = data[self.input_keys[1]]                  # [B, S]\n",
    "        \n",
    "        # Compute weighted mean\n",
    "        W_expanded = W.unsqueeze(-1)  # [B, S, 1]\n",
    "        mu_next = (W_expanded * sigma_points_next).sum(dim=1)  # [B, nx]\n",
    "        \n",
    "        result = {self.output_keys[0]: mu_next}\n",
    "        \n",
    "        if self.compute_variance:\n",
    "            # Compute weighted variance\n",
    "            diff = sigma_points_next - mu_next.unsqueeze(1)  # [B, S, nx]\n",
    "            var_next = (W_expanded * diff**2).sum(dim=1)     # [B, nx]\n",
    "            result[self.output_keys[1]] = var_next\n",
    "            \n",
    "        return result\n",
    "\n",
    "# ==============================================================================\n",
    "# SYSTEM CREATION\n",
    "# ==============================================================================\n",
    "\n",
    "def create_modular_ut_system(\n",
    "    dynamics_net: nn.Module,\n",
    "    nx: int = 1,\n",
    "    nu: int = 1,\n",
    "    dt: float = 0.01,\n",
    "    sigma_val: float = 0.1,\n",
    "    include_diffusion: bool = True,\n",
    "    compute_variance: bool = False,\n",
    "    method: str = 'dopri5',\n",
    "    rtol: float = 1e-5,\n",
    "    atol: float = 1e-7\n",
    ") -> System:\n",
    "    \"\"\"\n",
    "    Create complete modular UT system.\n",
    "    \n",
    "    Args:\n",
    "        dynamics_net: Neural network f(x, u) -> dx\n",
    "        nx: State dimension\n",
    "        nu: Control dimension  \n",
    "        dt: Integration time step\n",
    "        sigma_val: Diffusion strength\n",
    "        include_diffusion: Whether to include stochastic diffusion\n",
    "        compute_variance: Whether to compute output variance\n",
    "        method: ODE integration method\n",
    "        rtol: Relative tolerance\n",
    "        atol: Absolute tolerance\n",
    "        \n",
    "    Returns:\n",
    "        Complete System: mu, var, u -> mu_next, [var_next]\n",
    "    \"\"\"\n",
    "\n",
    "    rhs_fx = blocks.MLP(nx+nu, nx, ...)\n",
    "    \n",
    "    # Node 1: Generate sigma points\n",
    "    sigma_gen_func = SigmaPointGeneratorNode(\n",
    "        include_diffusion=include_diffusion,\n",
    "       \n",
    "    )\n",
    "\n",
    "    sigma_gen_node = Node( sigma_gen_func, \n",
    "                            input_keys=['mu', 'var', 'u'],\n",
    "                            output_keys=['sigma_points', 'W']\n",
    "                        )\n",
    "    \n",
    "    # Node 2: Apply dynamics with UTIntegrator\n",
    "    ut_integrator = UTIntegrator(\n",
    "        dynamics_net=rhs_fx,\n",
    "        dt=dt,\n",
    "        method=method,\n",
    "        rtol=rtol,\n",
    "        atol=atol,\n",
    "        sigma_val=sigma_val,\n",
    "        include_diffusion=include_diffusion,\n",
    "        nx=nx,\n",
    "        nu=nu\n",
    "    )\n",
    "\n",
    "    integrator_node = Node(ut_integrator, input_keys=['sigma_points', 'W], output_keys=['sigma_points_next'])\n",
    "    \n",
    "\n",
    "    \n",
    "    # Node 3: Reconstruct moments\n",
    "    inverse_sigma = InverseSigmaPointNode(\n",
    "        compute_variance=compute_variance,\n",
    "        input_keys=['sigma_points_next', 'W'],\n",
    "        output_keys=['mu_next'] + (['var_next'] if compute_variance else [])\n",
    "    )\n",
    "\n",
    "    inverse_sigma_node = Node(InverseSigmaPointNode, \n",
    "                        input_keys=['sigma_points_next', 'W'],\n",
    "                        output_keys=['mu', 'var'])\n",
    "    \n",
    "    # Assemble system\n",
    "    nodes = [sigma_gen_node, integrator_node, inverse_sigma_node]\n",
    "    system = System(nodes, name='modular_ut_system')\n",
    "    \n",
    "    return system\n",
    "\n",
    "# ==============================================================================\n",
    "# CONFIGURATION\n",
    "# ==============================================================================\n",
    "\n",
    "@dataclass\n",
    "class UTSystemConfig:\n",
    "    \"\"\"Configuration for UT system.\"\"\"\n",
    "    # System parameters\n",
    "    nx: int = 1              # State dimension\n",
    "    nu: int = 1              # Control dimension\n",
    "    dt: float = 0.01         # Time step\n",
    "    sigma_val: float = 0.1   # Diffusion strength\n",
    "    include_diffusion: bool = True\n",
    "    compute_variance: bool = False\n",
    "    \n",
    "    # Integration parameters\n",
    "    method: str = 'dopri5'   # ODE method\n",
    "    rtol: float = 1e-5       # Relative tolerance\n",
    "    atol: float = 1e-7       # Absolute tolerance\n",
    "    \n",
    "    # Network parameters\n",
    "    hidden_dim: int = 64     # Hidden layer size\n",
    "    n_layers: int = 2        # Number of hidden layers\n",
    "    \n",
    "    # Training parameters\n",
    "    batch_size: int = 128\n",
    "    epochs: int = 20\n",
    "    lr: float = 1e-3\n",
    "    train_frac: float = 0.7\n",
    "    val_frac: float = 0.15\n",
    "    device: str = \"cpu\"\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
